---
title: "tick_searches"
author: "Ilya"
date: "5/13/2018"
output: github_document
always_allow_html: yes
---

####When we go outdoors, we run a risk of coming into contact with ticks that could give us disease-causing pathogens. In the eastern U.S., blacklegged ticks (Ixodes scapularis) transmit the bacterium Borrelia burgdorferi, which causes Lyme disease; in the West Coast, the Western blacklegged tick (Ixodes pacificus) is the vector for Lyme disease. Both of these ticks also transmit pathogens causing other diseases in people and pets. Tick activity varies depending on time of year, weather, wildlife host abundance, and other factors. Knowing the level of risk in our area at a particular time can help us decide what outdoor activities to pursue and what precautions to take (such as checking oneself for ticks after going into tick habitat).  

####Internet search data offer a potential source of real-time information on people's encounters with ticks. If we know people in our area are searching more for ticks, this could be a sign that tick activity is high and we need to be more vigilant against ticks. In evaluating whether internet search data provide a useful measure of disease risk, an important step is determining whether internet search predicts Lyme disease incidence. Here we use Google trends data from 2004 to 2016 to predict Lyme disease incidence in those years. The analysis demonstrates that top searches related to tick bites account for 50% of the variance in Lyme disease incidence. Based on this finding, Google search data appears to be a useful measure of encountering ticks that make us sick. A next step will be identifying variables (e.g., weather) that predict increases in Google searches about tick bites, to enable earlier warning of tick risk.      

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#install packages
```{r}
pkgTest <- function(x)
{
  if (x %in% rownames(installed.packages()) == FALSE) {
    install.packages(x, dependencies= TRUE)    
  }
  library(x, character.only = TRUE)
}
neededPackages <- c("sp", "raster", "leaflet", "geojsonio", "lubridate", "data.table", "devtools", "dplyr", "gbm", "caret", "cowplot", "ggplot2", 
                      "ggstance")
for (package in neededPackages){pkgTest(package)}

library(devtools)
devtools::install_github("PMassicotte/gtrendsR", branch = "low-search-volume") #use version for getting low search volume regions https://github.com/PMassicotte/gtrendsR/issues/229
library(gtrendsR) 

```

#read in geojson file with nielsen dmas  
```{r}
#https://rstudio.github.io/leaflet/json.html
substrRight <- function(x, n){
  substr(x, nchar(x)-n+1, nchar(x))
}

#nielsen DMA
dma <- geojsonio::geojson_read("https://rawgit.com/simzou/nielsen-dma/master/nielsentopo.json",
  what = "sp")

#make map of dmas to make sure they look okay
pal <- colorNumeric("viridis", NULL)

#commenting this out because it does not display well in github_document
# leaflet(dma) %>%
#   addTiles() %>%
#   addPolygons(stroke = FALSE, smoothFactor = 0.3, fillOpacity = 1,
#     fillColor = ~pal(log10(cableperc)),
#     label = ~paste0(name, ": ", formatC(cableperc, big.mark = ","))) %>%
#   addLegend(pal = pal, values = ~log10(cableperc), opacity = 1.0,
#     labFormat = labelFormat(transform = function(x) round(10^x)))

#sort dma by dma1
dma.df = as.data.frame(dma)
dma.df$dma1 = as.character(dma.df$dma1)
dma.df$state = substrRight(dma.df$dma1, 2)

dma.df = dma.df[order(dma.df$state, dma.df$dma1),]
save(dma.df, file ="dma.df.Rdata")
write.csv(dma.df, file = "dma.csv")

```

#get google trends data
#####Note: additional terms may be useful for predicting Lyme disease (e.g., outdoor activities, symptoms, "Lyme disease", wildlife host community). But for this analysis the focus is on terms that reflect encounters with ticks. We used the term "tick bite", and the top 10 related queries for that term. We excluded queries related to tick species other than the blacklegged tick (Ixodes scapularis), and related to tick encounters with pets. We also excluded queries such as "what does tick bite look like", that contain within it other search terms already included (in this case, "tick bite").  
```{r}

#make function for substring
substrRight <- function(x, n){
  substr(x, nchar(x)-n+1, nchar(x))
}

years = seq(from = 2004, to = 2016, by = 1)
year.lag.1 = seq(from = 2004, to = 2015, by =1)
year.lag.2 = seq(from = 2004, to = 2014, by =1)
# year.lag.1 = seq(from = 2004, to = 2015, by =1)

start.month.day = rep("01-01",length(years))
end.month.day = rep("12-31", length(years))
start.days = paste(as.character(years), start.month.day, sep="-")
end.days = paste(as.character(years), end.month.day, sep="-")
start.days.lag1 = paste(as.character(year.lag.1), start.month.day, sep="-")
end.days.lag1 = paste(as.character(year.lag.1), end.month.day, sep="-")

start.days.lag2 = paste(as.character(year.lag.2), start.month.day, sep="-")
end.days.lag2 = paste(as.character(year.lag.2), end.month.day, sep="-")

out = NULL
a = 1
#for (a in 1){
for (a in 1:length(years)){
  print(a)
  gt <- gtrends(keyword = "tick bite", geo = c("US"), time = paste(start.days[a], end.days[a], sep = " "), gprop = c("web"), category = 0, hl = "en-US",
  low_search_volume = TRUE)
  gt$interest_by_dma$state = substrRight(gt$interest_by_dma$location, 2)
  gt$interest_by_dma$year = years[a]
  gt = gt$interest_by_dma
    gt = gt[order(gt$location),]
    gt$tick.bite = gt$hits
    gt = gt[,c("location","tick.bite", "state", "year")]

      #deer tick
            gt.deer.tick <- gtrends(keyword = "deer tick", geo = c("US"), time = paste(start.days.lag1[a], end.days.lag1[a], sep = " "), gprop = c("web"), category = 0, hl = "en-US",
                       low_search_volume = TRUE)
    gt.deer.tick$interest_by_dma$state = substrRight(gt.deer.tick$interest_by_dma$location, 2)
    gt.deer.tick$interest_by_dma$year = year.lag.2[a]+2
    gt.deer.tick = gt.deer.tick$interest_by_dma
    gt.deer.tick = gt.deer.tick[order(gt.deer.tick$location),]
    gt.deer.tick$deer.tick = gt.deer.tick$hits
    gt.deer.tick = gt.deer.tick[,c("location","deer.tick", "state", "year")]
    #now add to gt
    gt.deer.tick$location == gt$location
    gt$deer.tick = gt.deer.tick$deer.tick
    rm(gt.deer.tick)

      #tick.bites 
      gt.tick.bites <- gtrends(keyword = "tick bites", geo = c("US"), time = paste(start.days.lag1[a], end.days.lag1[a], sep = " "), gprop = c("web"), category = 0, hl = "en-US",
                              low_search_volume = TRUE)
      gt.tick.bites$interest_by_dma$state = substrRight(gt.tick.bites$interest_by_dma$location, 2)
      gt.tick.bites$interest_by_dma$year = year.lag.2[a]+2
      gt.tick.bites = gt.tick.bites$interest_by_dma
      gt.tick.bites = gt.tick.bites[order(gt.tick.bites$location),]
      gt.tick.bites$tick.bites = gt.tick.bites$hits
      gt.tick.bites = gt.tick.bites[,c("location","tick.bites", "state", "year")]
      #now add to gt
      gt.tick.bites$location == gt$location
      gt$tick.bites = gt.tick.bites$tick.bites
      rm(gt.tick.bites)
      
      #ticks 
      gt.ticks <- gtrends(keyword = "ticks", geo = c("US"), time = paste(start.days.lag1[a], end.days.lag1[a], sep = " "), gprop = c("web"), category = 0, hl = "en-US",
                              low_search_volume = TRUE)
      gt.ticks$interest_by_dma$state = substrRight(gt.ticks$interest_by_dma$location, 2)
      gt.ticks$interest_by_dma$year = year.lag.2[a]+2
      gt.ticks = gt.ticks$interest_by_dma
      gt.ticks = gt.ticks[order(gt.ticks$location),]
      gt.ticks$ticks = gt.ticks$hits
      gt.ticks = gt.ticks[,c("location","ticks", "state", "year")]
      #now add to gt
      gt.ticks$location == gt$location
      gt$ticks = gt.ticks$ticks
      rm(gt.ticks)

      #repellent 
    gt.repellent <- gtrends(keyword = "repellent", geo = c("US"), time = paste(start.days.lag1[a], end.days.lag1[a], sep = " "), gprop = c("web"), category = 0, hl = "en-US",
                        low_search_volume = TRUE)
    gt.repellent$interest_by_dma$state = substrRight(gt.repellent$interest_by_dma$location, 2)
    gt.repellent$interest_by_dma$year = year.lag.2[a]+2
    gt.repellent = gt.repellent$interest_by_dma
    gt.repellent = gt.repellent[order(gt.repellent$location),]
    gt.repellent$repellent = gt.repellent$hits
    gt.repellent = gt.repellent[,c("location","repellent", "state", "year")]
    #now add to gt
    gt.repellent$location == gt$location
    gt$repellent = gt.repellent$repellent
    rm(gt.repellent)

    #hiking 
    gt.hiking <- gtrends(keyword = "hiking", geo = c("US"), time = paste(start.days.lag1[a], end.days.lag1[a], sep = " "), gprop = c("web"), category = 0, hl = "en-US",
                        low_search_volume = TRUE)
    gt.hiking$interest_by_dma$state = substrRight(gt.hiking$interest_by_dma$location, 2)
    gt.hiking$interest_by_dma$year = year.lag.2[a]+2
    gt.hiking = gt.hiking$interest_by_dma
    gt.hiking = gt.hiking[order(gt.hiking$location),]
    gt.hiking$hiking = gt.hiking$hits
    gt.hiking = gt.hiking[,c("location","hiking", "state", "year")]
    #now add to gt
    gt.hiking$location == gt$location
    gt$hiking = gt.hiking$hiking
    rm(gt.hiking)

    #deet 
    gt.deet <- gtrends(keyword = "deet", geo = c("US"), time = paste(start.days.lag1[a], end.days.lag1[a], sep = " "), gprop = c("web"), category = 0, hl = "en-US",
                        low_search_volume = TRUE)
    gt.deet$interest_by_dma$state = substrRight(gt.deet$interest_by_dma$location, 2)
    gt.deet$interest_by_dma$year = year.lag.2[a]+2
    gt.deet = gt.deet$interest_by_dma
    gt.deet = gt.deet[order(gt.deet$location),]
    gt.deet$deet = gt.deet$hits
    gt.deet = gt.deet[,c("location","deet", "state", "year")]
    #now add to gt
    gt.deet$location == gt$location
    gt$deet = gt.deet$deet
    rm(gt.deet)
    out = rbind(out, gt)


}
gt = out
save(gt, file = "gt.Rdata")

#output one year/keyword subset for assigning google dma names to dma names in json file (they are slightly different)
dma.tab = subset(gt, year ==2004) 
dma.tab = dma.tab[order(dma.tab$state),]
write.csv(dma.tab, file = "dma.tab.google.csv")
```

#assign google dma names to dma-county file (do this part in excel, pseudo code here)
```{r}
#next: 
#open dma.csv and  dma.tab.google.csv in excel, 
#copy in google trends dma names into dma.csv, 
#align the two dma name sets, fixing state abbreviations, 
#and save dma.csv as dma.google.assigned.by.hand.csv
```

#read dma json file back in after assigning google trends dma names to it and fixing state abbreviations
```{r}
load("dma.df.Rdata")
dma.assigned = read.csv( "dma.google.assigned.by.hand.csv")
dma.assigned = dma.assigned[order(dma.assigned$id),]
dma.df = dma.df[order(dma.df$id),]
#check these are all true as error check nothing odd happened
dma.df$dma1 ==dma.assigned$dma1
#fix DC
ind.dc = which(dma.assigned$state == "D)")
dma.assigned$state = as.character(dma.assigned$state)
dma.assigned$state[ind.dc]= "DC"
dma.ggl <- dma.assigned
keep.col = c("latitude","dma","dma1", "longitude", "state", "google.dma")
dma.ggl = dma.ggl[,keep.col]

save(dma.ggl, file = "dma.ggl.Rdata")
```

#now assign google dmas to counties
```{r}
require(sp)
load("dma.ggl.Rdata")
#counties <- geojsonio::geojson_read("http://eric.clst.org/assets/wiki/uploads/Stuff/gz#_2010_us_050_00_500k.json",
#  what = "sp")

#https://catalog.data.gov/dataset/us-counties/resource/cc1b2e44-d5a4-4c26-82c1-39b0da37bfb8
counties.test <- shapefile("tl_2016_us_county.shp")
counties = counties.test
#confirm there is a projetion in counties
projection(counties)#there is none, but we're not going to use it

pal <- colorNumeric("viridis", NULL)

#http://eric.clst.org/tech/usgeojson/
#make map of counties to make sure they look like counties -- commented out to make markdown smaller
# leaflet(counties) %>%
#   addTiles() %>%
#   addPolygons(stroke = FALSE, smoothFactor = 0.3, fillOpacity = 1,
#     fillColor = ~pal((as.numeric(STATEFP))),
#     label = ~paste0(NAME, ": ", formatC(as.numeric(STATEFP), big.mark = ","))) %>%
#   addLegend(pal = pal, values = ~(as.numeric(STATEFP)), opacity = 1.0,
#     labFormat = labelFormat(transform = function(x) round(10^x)))

#read dma in again as polygons (need it to be polygons, not data.frame)
dma <- geojsonio::geojson_read("https://rawgit.com/simzou/nielsen-dma/master/nielsentopo.json",
  what = "sp")

#tried assigning a projection to dma, but it seemed to result in holes in polygons
# dma_project <- projection(dma, CRS("+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"))

#then assign google dma name
#male version of dma.ggl w/ only relevant columns
dma.ggl.sm = dma.ggl[,c("dma1","google.dma")]
#merge dma with dma.ggl.sm, assigning google dma name
#http://www.nickeubank.com/wp-content/uploads/2015/10/RGIS2_MergingSpatialData_part1_Joins.html#spatial-non-spatial
#doing this with merge messes it up w/ respect to holes in polygons, so instead use for loop
# dma = merge(dma,dma.ggl.sm, by.x = "dma1", by.y = "dma1")
dma$dma.google = NA
udma = unique(dma$dma1)
a = 1
for (a in 1:length(udma)){
  ind = which(dma$dma1==udma[a])
  ind.google = which(dma.ggl.sm$dma1==udma[a])
  dma$dma.google[ind]= as.character(dma.ggl.sm$google.dma[ind.google])
}

#counties has a projection; dma does not; assigning no projection to counties
projection(counties) <- projection(dma)#package raster

#this does not work if identical CRS(x,y) is not true
counties$dma.ggl <- over( counties, dma)$dma.google#name of dma

#read in fps codes
fps = fread("https://www2.census.gov/geo/docs/reference/state.txt")
fps = fps[,c("STATE", "STUSAB", "STATE_NAME")]
names(fps)[names(fps)=="STATE"]="STATEFP"

counties$STATEFP = as.numeric(as.character(counties$STATEFP))
counties = merge(counties,fps, by.x = "STATEFP", by.y = "STATEFP")
save(counties, file = "counties.Rdata")

#used this below to check for holes, and found them in dma when it had a projection assigned (which resulted in function over failing)
# require(devtools)
# install_github("eblondel/cleangeo")
# require(cleangeo)
# report <- clgeo_CollectionReport(counties)
# summary <- clgeo_SummaryReport(report)
# issues <- report[report$valid == FALSE,]
# 
# report <- clgeo_CollectionReport(dma)
# summary <- clgeo_SummaryReport(report)
# issues <- report[report$valid == FALSE,]
```


#read in CDC Lyme data and reshape  
```{r}
#website: https://www.cdc.gov/lyme/stats/index.html
L = read.csv("https://www.cdc.gov/lyme/resources/ld-Case-Counts-by-County-00-16.csv")
L = reshape(L, direction = "long", varying = 5:21, sep = "")
dim(L)
save(L, file = "L.Rdata")
```


#read in census data from SEER.  
```{r}

library(data.table)
#source: https://seer.cancer.gov/popdata/download.html
#dictionary for data: https://seer.cancer.gov/popdata/popdic.html
A = read.table("us.1990_2016.19ages.adjusted.txt", skip = 1)
A=as.data.frame(A)
A$V1 = as.character(A$V1)
A$year = substr(A$V1, 1, 4)
A$state = substr(A$V1, 5, 6)
A$county_fps = substr(A$V1, 9, 11)
A$population = substr(A$V1, 19, 26)
save(A, file = "A.Rdata")
# 
```

#fix population in census data
```{r}
library(data.table)
load("A.Rdata") 
A$population.2 =  as.numeric(substr(A$population,regexpr("[^0]",A$population),nchar(A$population)))
 A$population = A$population.2
 keep.col = c("V1", "year", "state", "county_fps", "population")
A = A[, keep.col]

# #county fps
 fps = fread("https://www2.census.gov/geo/docs/reference/codes/files/national_county.txt")
 names(fps)= c("state", "statefp", "county_fps", "county", "classfp")
 fps = data.frame(fps)
 keep.col = c("state", "county", "county_fps")
 fps = fps[, keep.col]
 A$county_fps=as.numeric(as.character(A$county_fps))

 test = setdiff(A$county_fps, fps$county_fps )
 A.test = subset(A, county_fps %in% test)
 #A.test$county_fps = as.numeric(as.character(A.test$county_fps))
 #merge to add county field
 A.test$county = NA
 #A.test2 = merge(A.test, fps, by = intersect(names(A.test), names(fps)))
  A2 = merge(A, fps, by = intersect(names(A), names(fps)))
#
 A3 = rbind(A2, A.test)
#
 A = A3
#
 A$year = as.numeric(A$year)
 A$population =  as.numeric(substr(A$population,regexpr("[^0]",A$population),nchar(A$population)))
 A2 = A
 save(A2, file = "A2.Rdata")

```

#read in Census data and summarize by year
```{r}
load("A2.Rdata")
A = A2
keep = c("state", "population", "county", "year")
A1 = A[,keep]

A1$county_state = paste(A1$county, A1$state, sep = "_")
A2 <- A1 %>%
  group_by(county_state, year) %>%
  summarize(population = sum(population),
            county = county[1], 
            state = state[1])
census = A2
save(census, file = "census.Rdata")

```


#read back in Census data, reshape long, assign full names of states
```{r}
load("census.Rdata")
#names(census)[names(census)=="state"]="state_name"
names(census)[names(census)=="county"]="county_name"
census = subset(census, year >=2004)

#read in file of state abbreviations
states = read.csv("50_us_states_all_data.csv")
states = states[,c(2:3)]
colnames(states) =c("state_name", "state")

intersect(names(census), names(states))
#add state abbreviation to census data
census1 = merge(census, states, by = "state")
#check a few rows
census = census1
head(census1)
tail(census1)
length(unique(census$year))
save(census, file = "census.Rdata")

```

#combine census and LD data
```{r}
load("census.Rdata")
load("L.Rdata")
#make county_name and county_state
names(L)[names(L)=="Ctyname"]="county_name"
names(L)[names(L)=="Stname"]="state_name"
names(L)[names(L)=="time"]="year"
L$county_state =paste(L$county_name, L$state_name, sep = " ")
L = L[, c("county_name", "county_state", "state_name", "year", "Cases")]

test  = subset(census, is.na(population))#should be empty
#census = census[, c("state_name", "county_name", "year", "population", "state", "county_state")]
census$county_state=paste(census$county_name, census$state_name, sep = " ")

length(setdiff(L$county_state,census$county_state))
length(setdiff(census$county_state,L$county_state))
#seems to be missing Alabama, but that's okay as it's way South so doesn't have much Lyme 
L1 = merge(census, L, by = intersect(names(census), names(L)))
L = L1
L$incidence = 100000*L$Cases/L$population
length(unique(L$year))
save(L, file = "L.Rdata") 
```


#assign DMA and google trends to Lyme disease data
```{r}
require(sp)
# library(reshape2)
library(tidyr)
load("L.Rdata")#Lyme
load("gt.Rdata")
load("counties.Rdata")
names(counties)[names(counties)=="NAMELSAD"]="county_name"
names(counties)[names(counties)=="STUSAB"]="state"
counties$county_state =paste(counties$county_name, counties$STATE_NAME, sep = " ")
keep.col = c("dma.ggl", "county_state", "STATEFP"      ,"COUNTYFP", "county_name", "INTPTLAT", "INTPTLON","state"     ,  "STATE_NAME"    )
counties = counties[,keep.col]
counties.df = data.frame(counties)
names(counties.df)=tolower(names(counties.df))

#lose some AL counties here
Lc = merge(L,counties.df)

#assign google trends to L
gt = subset(gt, !is.na(gt$year))
names(gt)[names(gt)=="location"]="dma.ggl"
gt = gt[,c("dma.ggl", "year", "deer.tick", "tick.bite", "ticks", "tick.bites", "repellent", "deet", "hiking")]
L1 = merge(Lc, gt, by = c("dma.ggl", "year"))

L = subset(L1, !is.na(incidence))
save(L, file = "L.Rdata")
```

#summarize by dma
```{r}
load("L.Rdata")
library(dplyr)
L1<-L %>%
  group_by(dma.ggl, year) %>%
  summarize(pop.dma = sum(population),
            Cases.dma = sum(Cases),
            incidence.dma = 100000*Cases.dma/pop.dma,
            deer.tick = deer.tick[1],
            tick.bite = tick.bite[1],
            tick.bites=tick.bites[1],
            ticks = ticks[1], 
            repellent = repellent[1],
            hiking = hiking[1],
            deet = deet[1],
            state = state[1])
summary(L1$incidence.dma)
#L <- L1
save(L1, file = "L1.Rdata")
```


#run boosted regression tree analysis -- analyze at dma-level 
```{r}
set.seed(1234)

load("L1.Rdata")

#library(dismo)
L = L1
L$deer.tick = as.numeric(as.character(L$deer.tick))
L$dma.ggl = factor(L$dma.ggl)
L = data.frame(L)#have to change back from tibble to data.frame!!
L$state = factor(L$state)
keep.col = c("incidence.dma", "deer.tick", "tick.bite", "ticks", "tick.bites", "hiking", "deet", "repellent")


DP =createDataPartition(L1$Cases.dma, p = 0.8)
Train = L[DP$Resample1,]
Test = L[-DP$Resample1,]
save(Test, file = "Test.Rdata")

ntrees = 3000
gbm.dma = gbm(data=Train,
                            Cases.dma ~deer.tick + tick.bite+ ticks+ tick.bites+pop.dma,
                            distribution = "poisson",#default
                            n.trees = ntrees,#fit up to two-way interactions
                            shrinkage = 0.01,
                            interaction.depth = 4,
                            bag.fraction = 0.5)#default 

print(1-sum((Train$Cases.dma - predict(gbm.dma, newdata=Train, n.trees =ntrees,
                                         type="response"))^2)/
        sum((Train$Cases.dma - mean(Train$Cases.dma))^2))

#check names of columns used as predictors: 
save(gbm.dma, file = "gbm.dma.Rdata")
x = summary(gbm.dma)

#write results to csv
x.df= data.frame(variable = x$var, 
                 relative.influence = x$rel.inf)
x.df$relative.influence = round(x.df$relative.influence, digits = 3)
write.csv(x.df, file = "relative.influence.csv")
#check number of trees
ntrees = gbm.dma$n.trees
print(ntrees)#made 
print(x.df)
ind = which(x.df$variable == "pop.dma")
variable = as.character(x.df$variable)
variable[ind]=as.character("population")
x.df$variable = variable
search = rep("Google.search", dim(x.df)[1])
search[x.df$variable == "population"]="population"
x.df$search = factor(search) 
ggplot(data = x.df, aes(x =variable, y = relative.influence, fill = search))+
#  plot.tmp = ggplot()+

  geom_bar(stat="identity")
ggsave("Figure.search.jpg")
```
